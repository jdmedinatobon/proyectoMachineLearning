1) Prioridad: Preprocesamiento
Implica:
Nota: Mirar si toca remover el tejido no cerebral antes de todo el proceso (Usar una herramienta como BET de Oxford).
a) Leer y organizar los datos en una matriz de (64,64,30,1800/2700) 
b) Con los datos de a) sacar los features (Número tentativo: 21 features) y escribir estos en un archivo de texto.
Para sacar los features del GLCM se esta usando la libreria de mahotas. Tiene un problema y es que totea el uso de RAM de Google Colab y
termina la ejecucion del codigo. Para esto lo que hicimos fue reducir la escala de las imagenes, es decir, si utiliza una escala de grises de 65536 la reducimos hasta valores entre 16384 o 256. Entre mas bajo menos se demora, pero menor es la informacion de la imagen. Pendiente hablarlo con Fernando o buscar en internet sobre esto.

c) Con los datos preprocesados en features realizar nuevamente un preprocesamiento utilizando Blanqueamiento, estandarización, normalización, PCA, etc.
d) Vectorizar los datos preprocesados en un vector de (21/lo que sea, 1800/2700). Fin de preprocesamiento (O no?).

Plazo: Máximo el Lunes 11 de Noviembre tener esto terminado.

Nota: Preguntarle a Fernando sobre el objetivo del proyecto. Por ejemplo, si se quiere que el error sea muy bajito o que.

2) Entrenamiento del modelo.
La idea es usar AdaBoost y el objetivo principal por ahora es determinar un modelo.
Si queda tiempo y los tiempos de entrenamiento no son muy largos, se puede explorar realizar una comparación entre modelos
utilizando k-fold Cross-Validation (por lo que no hay tantos datos).
Implica:
a) Escoger un modelo para usar en AdaBoost (explorar la posibilidad de Kernels). Puede ser una red neuronal mas simple, o 
SVM o lo que sea. Aquí toca investigar un poco. La idea es hacerlo multiclase.
b) Implementar en Google Colab el código necesario para entrenar el modelo de AdaBoost con una librería (cual?)a partir 
de cualquier modelo de a).
Plazo: Domingo 17 de Noviembre.

c) Tomar los datos preprocesados y entrenar el modelo y medir el tiempo de entrenamiento y el error de validación. 
Con esto tomar decisiones sobre entrenamientos adicionales con modelos diferentes, preprocesamientos distintos, kernels, etc..
Plazo: Domingo 24 de Noviembre, ojalá mañana.
